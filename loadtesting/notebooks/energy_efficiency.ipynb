{
 "cells": [
  {
   "cell_type": "code",
   "id": "f546290dcd056f13",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pytz\n",
    "import re\n",
    "import os\n",
    "import logging"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3d6de811535875d3",
   "metadata": {},
   "source": [
    "basedir = \"../data/baseapp/2024-07-15/16-18/\"\n",
    "outputdir = basedir+\"output/\"\n",
    "os.makedirs(outputdir, exist_ok=True)\n",
    "locust_data_history_file = basedir + \"data_stats_history.csv\"\n",
    "locust_data_file = basedir + \"data_stats.csv\"\n",
    "kepler_data_file = basedir+\"kepler.csv\"\n",
    "pdu_data_file = basedir+\"pdu.csv\"\n",
    "service_metrics_data_file = basedir+\"service_metrics.csv\"\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    filename=outputdir+\"run.log\",\n",
    "                    format='%(asctime)s %(levelname)s - %(message)s',\n",
    "                    datefmt='%H:%M:%S',\n",
    "                    filemode='a')\n",
    "logging.getLogger().addHandler(logging.StreamHandler())\n",
    "logging.info(\"Starting\")\n",
    "stages = [\n",
    "    {\"users\": 100, \"spawn_rate\": 1},\n",
    "    {\"users\": 200, \"spawn_rate\": 1},\n",
    "    {\"users\": 300, \"spawn_rate\": 1},\n",
    "    {\"users\": 400, \"spawn_rate\": 1},\n",
    "    {\"users\": 500, \"spawn_rate\": 1}\n",
    "]\n",
    "\n",
    "valid_users = set()\n",
    "for stage in stages:\n",
    "    valid_users.add(stage['users'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8f1126266d6019b3",
   "metadata": {},
   "source": [
    "locust_df = pd.read_csv(\"%s\" % locust_data_history_file)\n",
    "\n",
    "# timestamp to datetime\n",
    "locust_df['Timestamp'] = pd.to_datetime(locust_df['Timestamp'], unit='s')\n",
    "\n",
    "# convert datetime to europe berline timezone and use it as index\n",
    "germany_tz = pytz.timezone('Europe/Berlin')\n",
    "locust_df['Timestamp'] = locust_df['Timestamp'].dt.tz_localize('UTC').dt.tz_convert(germany_tz)\n",
    "locust_df.set_index('Timestamp', inplace=True)\n",
    "\n",
    "# Delete unrelevant data\n",
    "locust_df.drop(columns=['Type'], inplace=True)\n",
    "locust_df = locust_df[locust_df['Total Request Count'] != 0]\n",
    "\n",
    "# Only look at the \"wanted\" stages (not the step-up between stages)\n",
    "locust_df = locust_df[locust_df['User Count'].isin(valid_users)]\n",
    "locust_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get the min and max index (time) for the current stage\n",
    "start_time, end_time = locust_df.index.min(), locust_df.index.max()\n",
    "\n",
    "# Convert the timezoned datetime to naive datetime (UTC or localize as needed)\n",
    "start_time = start_time.tz_convert(\"Europe/Berlin\").tz_localize(None)\n",
    "end_time = end_time.tz_convert(\"Europe/Berlin\").tz_localize(None)\n",
    "logging.info(\"Start: %s ; End: %s\", start_time, end_time)"
   ],
   "id": "7096181dcc2cb44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Determine the appropriate unit for the total energy consumed\n",
    "def convert_energy(total_energy_joules):\n",
    "    if total_energy_joules >= 1e6:  # 1 Megajoule = 1,000,000 Joules\n",
    "        energy_unit = \"MJ\"\n",
    "        total_energy = total_energy_joules / 1e6\n",
    "    elif total_energy_joules >= 1e3:  # 1 Kilojoule = 1,000 Joules\n",
    "        energy_unit = \"kJ\"\n",
    "        total_energy = total_energy_joules / 1e3\n",
    "    else:\n",
    "        energy_unit = \"J\"\n",
    "        total_energy = total_energy_joules\n",
    "    return total_energy, energy_unit\n",
    "\n",
    "def transform_joules(energy, from_unit, to_unit):\n",
    "    # Conversion factors to Joules\n",
    "    conversion_to_joules = {\n",
    "        \"MJ\": 1e6,\n",
    "        \"kJ\": 1e3,\n",
    "        \"J\": 1\n",
    "    }\n",
    "    \n",
    "    if from_unit not in conversion_to_joules:\n",
    "        raise ValueError(f\"Unknown from_unit: {from_unit}\")\n",
    "    \n",
    "    if to_unit not in conversion_to_joules:\n",
    "        raise ValueError(f\"Unknown to_unit: {to_unit}\")\n",
    "    \n",
    "    # Convert from original unit to Joules\n",
    "    energy_in_joules = energy * conversion_to_joules[from_unit]\n",
    "    \n",
    "    # Convert from Joules to the target unit\n",
    "    energy_in_target_unit = energy_in_joules / conversion_to_joules[to_unit]\n",
    "    \n",
    "    return energy_in_target_unit"
   ],
   "id": "bda16a8d8a016bda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Calculate the total Energy consumption using Kepler & PDU Data\n",
    "\n",
    "Note 1: Kepler data consists of the kepler_container_joules_total metric, which tracks the aggregated/consumed amount of joules for each container\n",
    "If a container is stopped then the responding columns values have no values. The same for containers which are started in the future.\n",
    "To sum up the energy over the time we will start all columns which do not start with values with zero's and continue on missing values with the last seen value. "
   ],
   "id": "92bc3ac10f279c9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pdu_df = pd.read_csv(pdu_data_file,  parse_dates=['Time'])\n",
    "pdu_df['Time'] = pd.to_datetime(pdu_df['Time'])  # Ensure consistent timezone\n",
    "# Ensure the DataFrame is sorted by Time\n",
    "pdu_df = pdu_df.sort_values(by='Time')\n",
    "pdu_df.index = pd.to_datetime(pdu_df.index)\n",
    "\n",
    "# Function to remove ' W' and convert to numeric\n",
    "def strip_w_convert(series):\n",
    "    return pd.to_numeric(series.str.replace(' W', ''), errors='coerce')\n",
    "\n",
    "# Apply the function to all columns except 'Time'\n",
    "for column in pdu_df.columns:\n",
    "    if column != 'Time':\n",
    "        pdu_df[column] = strip_w_convert(pdu_df[column])\n",
    "\n",
    "def calculate_pdu_energy_consumption(pdu_df, start_time, end_time):\n",
    "    # Filter the DataFrame for the specified time range\n",
    "    time_filtered_df = pdu_df[(pdu_df['Time'] >= start_time) & (pdu_df['Time'] <= end_time)].copy()\n",
    "\n",
    "    if time_filtered_df.empty:\n",
    "        logging.warning(\"The filtered DataFrame is empty. Ensure the time range is within the data bounds.\")\n",
    "        return convert_energy(0)\n",
    "\n",
    "    # Calculate time difference between consecutive measurements in seconds\n",
    "    time_filtered_df.loc[:, 'Time_diff'] = time_filtered_df['Time'].diff().dt.total_seconds()\n",
    "\n",
    "    # Calculate the energy consumed during each interval (Power * Time_diff)\n",
    "    time_filtered_df['Energy_Joules'] = time_filtered_df['Value'] * time_filtered_df['Time_diff']\n",
    "    \n",
    "    # Summing up the energy consumed\n",
    "    total_energy_joules = time_filtered_df['Energy_Joules'].sum()\n",
    "\n",
    "    # Convert the total energy to the appropriate unit\n",
    "    return convert_energy(total_energy_joules)\n",
    "\n",
    "# Calculate energy consumption for the specified time range\n",
    "total_energy_pdu, unit_pdu = calculate_pdu_energy_consumption(pdu_df, start_time, end_time)\n",
    "\n",
    "logging.info(f\"Total energy consumed from {start_time} to {end_time} tracked by pdu metrics: {total_energy_pdu} {unit_pdu}\")\n"
   ],
   "id": "bfe9070eb154a3d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "kepler_df = pd.read_csv(kepler_data_file,  parse_dates=['Time'])\n",
    "kepler_df['Time'] = pd.to_datetime(kepler_df['Time'])  # Ensure consistent timezone\n",
    "# Ensure the DataFrame is sorted by Time\n",
    "kepler_df = kepler_df.sort_values(by='Time')\n",
    "kepler_df.index = pd.to_datetime(kepler_df.index)\n",
    "# see note 1\n",
    "for column in kepler_df.columns:\n",
    "    if column != 'Time':\n",
    "        kepler_df[column] = kepler_df[column].ffill().fillna(0)\n",
    "\n",
    "def calculate_kepler_energy_consumption(kepler_df, start_time, end_time):\n",
    "    # Ensure the DataFrame is sorted by Time\n",
    "    kepler_df = kepler_df.sort_values(by='Time')\n",
    "    \n",
    "    # Check if start_time is within the DataFrame's time range\n",
    "    if start_time < kepler_df['Time'].min():\n",
    "        logging.warning(f\"Start time {start_time} is before the first timestamp. Using the first available value.\")\n",
    "        start_values = kepler_df.iloc[0].drop(labels='Time')\n",
    "    else:\n",
    "        start_values = kepler_df.loc[kepler_df['Time'] >= start_time].iloc[0].drop(labels='Time')\n",
    "\n",
    "    # Check if end_time is within the DataFrame's time range\n",
    "    if end_time > kepler_df['Time'].max():\n",
    "        logging.warning(f\"End time {end_time} is after the last timestamp. Using the last available value.\")\n",
    "        end_values = kepler_df.iloc[-1].drop(labels='Time')\n",
    "    elif end_time<kepler_df['Time'].min():\n",
    "        return 0, \"J\"\n",
    "    else:\n",
    "        end_values = kepler_df.loc[kepler_df['Time'] <= end_time].iloc[-1].drop(labels='Time')\n",
    "\n",
    "    # Calculate the difference between end and start values\n",
    "    energy_difference = end_values.values - start_values.values\n",
    "\n",
    "    # Sum up the differences\n",
    "    total_energy_sum = energy_difference.sum()\n",
    "\n",
    "    return convert_energy(total_energy_sum)\n",
    "\n",
    "# Calculate energy consumption for the specified time range\n",
    "total_energy_kepler, unit_kepler = calculate_kepler_energy_consumption(kepler_df, start_time, end_time)\n",
    "\n",
    "logging.info(f\"Total energy consumed from {start_time} to {end_time} tracked by kepler: {total_energy_kepler} {unit_kepler}\")"
   ],
   "id": "6cfffd6fe8c9c2ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ensure the data is sorted by 'Time'\n",
    "kepler_df = kepler_df.sort_values('Time').reset_index(drop=True)\n",
    "pdu_df = pdu_df.sort_values('Time').reset_index(drop=True)\n",
    "# Merge the dataframes on the 'Time' column\n",
    "merged_df = pd.merge_asof(kepler_df, pdu_df, on='Time')\n",
    "\n",
    "# Fill any resulting NaN values after the merge\n",
    "merged_df.ffill(inplace=True)\n",
    "merged_df.bfill(inplace=True)\n",
    "\n",
    "# Extract only the necessary Kepler columns (assuming these contain energy in Joules)\n",
    "kepler_columns = [col for col in merged_df.columns if 'kepler' in col]\n",
    "\n",
    "# Sum up the Kepler energy values at each time step\n",
    "merged_df['Kepler_sum'] = merged_df[kepler_columns].sum(axis=1)\n",
    "\n",
    "# Calculate the PDU energy consumption (integrate power over time as given)\n",
    "# Assuming 'Value_pdu' contains power in Watts\n",
    "merged_df['Time_diff'] = merged_df['Time'].diff().dt.total_seconds().fillna(0)\n",
    "merged_df['PDU_energy'] = (merged_df['Value'] * merged_df['Time_diff']).cumsum()\n",
    "\n",
    "# Calculate the increase in energy consumed for each time interval\n",
    "merged_df['Kepler_energy_increase'] = merged_df['Kepler_sum'].diff().fillna(0)\n",
    "merged_df['PDU_energy_increase'] = merged_df['PDU_energy'].diff().fillna(0)\n",
    "\n",
    "# Calculate the absolute errors between Kepler and PDU energy increases\n",
    "merged_df['abs_error_energy'] = (merged_df['PDU_energy_increase'] - merged_df['Kepler_energy_increase']).abs()\n",
    "\n",
    "# Calculate the Mean Absolute Error (MAE) over all energy increases\n",
    "mae = merged_df['abs_error_energy'].mean()\n",
    "logging.info(f\"Mean Abs Error of Keplers Trends compared to PDU (MAE over all data point diffs): {mae:.2f} J\")"
   ],
   "id": "f045751adcb113ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ax = merged_df.plot(x='Time', y=['Kepler_energy_increase','PDU_energy_increase'], kind='line', grid=True)\n",
    "ax.figure.savefig(outputdir+'energy_diffs.png', bbox_inches='tight')"
   ],
   "id": "62a18168ca14514e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ax=merged_df.boxplot('abs_error_energy', widths=3, figsize=(2,6))\n",
    "ax.figure.savefig(outputdir+'boxplot_energy_diffs_error.png', bbox_inches='tight')"
   ],
   "id": "99364f2560de9493",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Calculate the energy consumption on each stage ",
   "id": "2274d350700109d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "kepler_energy = []\n",
    "pdu_energy = []\n",
    "user_counts = []\n",
    "\n",
    "# Loop through each stage\n",
    "for stage in stages:\n",
    "    stage_users = stage['users']\n",
    "    stage_df = locust_df[locust_df['User Count'] == stage_users]\n",
    "    \n",
    "    # Append zeros for empty DataFrame and continue\n",
    "    if stage_df.empty:\n",
    "        logging.warning(\"The filtered DataFrame is empty. Ensure the time range is within the data bounds.\")\n",
    "        kepler_energy.append(0)\n",
    "        pdu_energy.append(0)\n",
    "        user_counts.append(stage_users)\n",
    "        continue\n",
    "\n",
    "    # Get the min and max index (time) for the current stage\n",
    "    stage_start_time, stage_end_time = stage_df.index.min(), stage_df.index.max()\n",
    "    # Convert the timezoned datetime to naive datetime (UTC or localize as needed)\n",
    "    stage_start_time = stage_start_time.tz_convert(\"Europe/Berlin\").tz_localize(None)\n",
    "    end_time = stage_end_time.tz_convert(\"Europe/Berlin\").tz_localize(None)\n",
    "        \n",
    "    k_energy, k_unit = calculate_kepler_energy_consumption(kepler_df, stage_start_time, end_time)\n",
    "    p_energy, p_unit = calculate_pdu_energy_consumption(pdu_df, stage_start_time, end_time)\n",
    "        \n",
    "    logging.info(f\"Stage: {stage_users}, From: {stage_start_time}, To: {end_time}, Kepler: {k_energy} {k_unit}, PDU: {p_energy} {p_unit}\")\n",
    "    \n",
    "    kepler_energy.append(transform_joules(k_energy, k_unit, \"kJ\"))\n",
    "    pdu_energy.append(transform_joules(p_energy, p_unit, \"kJ\"))\n",
    "    user_counts.append(stage_users)"
   ],
   "id": "795e1c5247082d42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "# Plot lines with markers to make data points visible\n",
    "ax.plot(user_counts, kepler_energy, label='Kepler Energy Consumption', color='blue', marker='o', markersize=4, zorder=50)\n",
    "ax.plot(user_counts, pdu_energy, label='PDU Energy Consumption', color='green', marker='o', markersize=4, zorder=50)\n",
    "\n",
    "# Fill between for kepler energy\n",
    "ax.fill_between(user_counts, kepler_energy, color='blue', alpha=0.6, step='post', zorder=5)\n",
    "\n",
    "# Fill between for pdu energy\n",
    "ax.fill_between(user_counts, pdu_energy, color='green', alpha=0.6, step='post', zorder=1)\n",
    "\n",
    "# Additional Plot Settings\n",
    "ax.set_xlabel('User Count')\n",
    "ax.set_ylabel('Energy Consumption (KiloJoules)')\n",
    "#ax.set_title('Energy Consumption by User Count Stage (Kepler vs PDU)')\n",
    "# Set x-axis ticks and labels only to show user counts\n",
    "ax.set_xticks(user_counts)\n",
    "ax.set_xticklabels(user_counts)\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig(outputdir+'energy_consumption_by_stage.png', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "56ea1e879333850b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Calculate the usefully done work",
   "id": "c651c091098a2850"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_work_metrics_df():\n",
    "    prometheus_metrics_df = pd.read_csv(service_metrics_data_file, parse_dates=['Time'])\n",
    "    prometheus_metrics_df['Time'] = pd.to_datetime(prometheus_metrics_df['Time'])  # Ensure consistent timezone\n",
    "    # Ensure the DataFrame is sorted by Time\n",
    "    prometheus_metrics_df = prometheus_metrics_df.sort_values(by='Time')\n",
    "    prometheus_metrics_df.index = pd.to_datetime(prometheus_metrics_df.index)\n",
    "    for column in prometheus_metrics_df.columns:\n",
    "        if column != 'Time':\n",
    "            prometheus_metrics_df[column] = prometheus_metrics_df[column].ffill().fillna(0) # make values static on absence of data, not anymore absent    \n",
    "    \n",
    "    # Melt the DataFrame\n",
    "    long_df = pd.melt(prometheus_metrics_df, id_vars=['Time'], var_name='Metric', value_name='Count')\n",
    "    \n",
    "    # Extract the application name\n",
    "    def extract_application(metric):\n",
    "        match = re.search(r'application=\"([^\"]+)', metric)\n",
    "        return match.group(1) if match else None\n",
    "    \n",
    "    long_df['Application'] = long_df['Metric'].apply(extract_application)\n",
    "    \n",
    "    # Convert 'Count' to numeric, ensuring invalid parsing will result in NaN, then fill them with 0\n",
    "    long_df['Count'] = pd.to_numeric(long_df['Count'], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Group by 'Time' and 'Application', and sum the 'Count'\n",
    "    summed_df = long_df.groupby(['Time', 'Application'])['Count'].sum().reset_index()\n",
    "    \n",
    "    return summed_df.pivot(index='Time', columns='Application', values='Count').fillna(0)\n",
    "\n",
    "service_work_metrics_df = calculate_work_metrics_df()\n",
    "service_work_metrics_df"
   ],
   "id": "48d4c534aaa0dcbb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Calculate the energy efficiency\n",
    "\n",
    "Energy Efficiency = Useful Work / Consumed Energy"
   ],
   "id": "39491c05c342eff3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "energy_efficiency_kepler = []\n",
    "energy_efficiency_pdu = []\n",
    "successful_requests_per_stage = []\n",
    "\n",
    "# Loop through each stage\n",
    "for stage in stages:\n",
    "    stage_users = stage['users']\n",
    "    stage_df = locust_df[locust_df['User Count'] == stage_users]\n",
    "    \n",
    "    # Get the min and max index (time) for the current stage\n",
    "    stage_start_time, stage_end_time = stage_df.index.min(), stage_df.index.max()\n",
    "    # Convert the timezoned datetime to naive datetime (localize to 'Europe/Berlin')\n",
    "    stage_start_time = stage_start_time.tz_convert(\"Europe/Berlin\").tz_localize(None)\n",
    "    stage_end_time = stage_end_time.tz_convert(\"Europe/Berlin\").tz_localize(None)\n",
    "    \n",
    "    # Filter service_work_metrics_df for the specified time range\n",
    "    work_metrics_filtered = service_work_metrics_df[(service_work_metrics_df.index >= stage_start_time) & (service_work_metrics_df.index <= stage_end_time)]\n",
    "    \n",
    "    # Calculate successful requests\n",
    "    successful_requests = work_metrics_filtered.max().sum()- work_metrics_filtered.min().sum()\n",
    "    \n",
    "    # Calculate energy efficiency (successful requests per kJ consumed)\n",
    "    kepler_energy_consumed = kepler_energy[user_counts.index(stage_users)]\n",
    "    pdu_energy_consumed = pdu_energy[user_counts.index(stage_users)]\n",
    "    \n",
    "    k_energy_efficiency = successful_requests / kepler_energy_consumed if kepler_energy_consumed != 0 else 0\n",
    "    pdu_energy_efficiency = successful_requests / pdu_energy_consumed if pdu_energy_consumed != 0 else 0\n",
    "    \n",
    "    # Append results\n",
    "    energy_efficiency_kepler.append(k_energy_efficiency)\n",
    "    energy_efficiency_pdu.append(pdu_energy_efficiency)\n",
    "    successful_requests_per_stage.append(successful_requests)\n",
    "    logging.info(f\"Stage: {stage_users}, From: {stage_start_time}, To: {stage_end_time}, Successfull Requests: {successful_requests} ,Kepler efficiency: {k_energy_efficiency}, PDU efficiency: {pdu_energy_efficiency}\")\n",
    "\n",
    "# Calculate total energy efficiency\n",
    "total_successful_requests = sum(successful_requests_per_stage)\n",
    "total_energy_kepler_kj = sum(kepler_energy)\n",
    "total_energy_pdu_kj = sum(pdu_energy)\n",
    "\n",
    "total_efficiency_kepler = total_successful_requests / total_energy_kepler_kj\n",
    "total_efficiency_pdu = total_successful_requests / total_energy_pdu_kj\n",
    "\n",
    "logging.info(\"Total Energy Efficiency (Kepler): %s successful requests/kJ\", total_efficiency_kepler)\n",
    "logging.info(\"Total Energy Efficiency (PDU): %s successful requests/kJ\", total_efficiency_pdu)\n",
    "\n",
    "total_efficiency_kepler = total_successful_requests / transform_joules(total_energy_kepler_kj,\"kJ\",\"J\")\n",
    "total_efficiency_pdu = total_successful_requests / transform_joules(total_energy_pdu_kj,\"kJ\",\"J\")\n",
    "\n",
    "logging.info(\"Total Energy Efficiency (Kepler): %s successful requests/J\", total_efficiency_kepler)\n",
    "logging.info(\"Total Energy Efficiency (PDU): %s successful requests/J\", total_efficiency_pdu)"
   ],
   "id": "db4b95d2d0344693",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax1 = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('User Count')\n",
    "ax1.set_ylabel('Energy Efficiency', color=color)\n",
    "\n",
    "# Plot energy efficiency\n",
    "ax1.plot(user_counts, energy_efficiency_kepler, label='Kepler Energy Efficiency', color='blue', marker='o', markersize=4)\n",
    "ax1.plot(user_counts, energy_efficiency_pdu, label='PDU Energy Efficiency', color='green', marker='o', markersize=4)\n",
    "\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.set_xticks(user_counts)\n",
    "ax1.set_xticklabels(user_counts)\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.grid()\n",
    "\n",
    "# Ensure the y-axis starts at 0\n",
    "ax1.set_ylim(bottom=0)\n",
    "\n",
    "# Create second y-axis\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:red'\n",
    "ax2.set_ylabel('Successful Requests', color=color)\n",
    "\n",
    "# Plot successful requests\n",
    "ax2.plot(user_counts, successful_requests_per_stage, label='Successful Requests', color='red', marker='s', linestyle='--')\n",
    "\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.set_xticks(user_counts)\n",
    "ax2.set_xticklabels(user_counts)\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Ensure the y-axis starts at 0\n",
    "ax2.set_ylim(bottom=0)\n",
    "\n",
    "#plt.title('Energy Efficiency and Successful Requests by User Count Stage')\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig(outputdir+'energy_efficiency_by_stage.png', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "8313594c7b9afe7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Calculate the energy efficiency of each service\n",
    "\n",
    "as we have seen now how Kepler's trend of energy follows the pdu graph, we can use Kepler's data to look into our black box system and look at each service's energy efficiency."
   ],
   "id": "50ffd415a193e9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Filtere die DataFrame-Spalten, die `container_namespace=\"default\"` enthalten\n",
    "namespace_default_columns = [col for col in kepler_df.columns if 'container_namespace=\"default\"' in col]\n",
    "\n",
    "# Extrahiere den `container_name` aus den Headern und speichere ihn in einer neuen DataFrame-Spalte\n",
    "service_names = [col.split('container_name=\"')[1].split('\"')[0] for col in namespace_default_columns]\n",
    "service_names = set(service_names)\n",
    "\n",
    "def calculate_service_energy_consumption(kepler_df, service_name, start_time, end_time):\n",
    "    # Ensure the DataFrame is sorted by Time\n",
    "    kepler_df = kepler_df.sort_values(by='Time')\n",
    "    \n",
    "    # Filter only the columns related to the specific service\n",
    "    service_columns = [col for col in kepler_df.columns if f'container_name=\"{service_name}\"' in col]\n",
    "    \n",
    "    if not service_columns:\n",
    "        logging.warning(f\"No data available for service: {service_name}\")\n",
    "        return 0, \"J\"\n",
    "    \n",
    "    # Check if start_time is within the DataFrame's time range\n",
    "    if start_time < kepler_df['Time'].min():\n",
    "        logging.warning(f\"Start time {start_time} is before the first timestamp. Using the first available value.\")\n",
    "        start_values = kepler_df.iloc[0][service_columns]\n",
    "    else:\n",
    "        start_values = kepler_df.loc[kepler_df['Time'] >= start_time].iloc[0][service_columns]\n",
    "\n",
    "    # Check if end_time is within the DataFrame's time range\n",
    "    if end_time > kepler_df['Time'].max():\n",
    "        logging.warning(f\"End time {end_time} is after the last timestamp. Using the last available value.\")\n",
    "        end_values = kepler_df.iloc[-1][service_columns]\n",
    "    elif end_time < kepler_df['Time'].min():\n",
    "        return 0, \"J\"\n",
    "    else:\n",
    "        end_values = kepler_df.loc[kepler_df['Time'] <= end_time].iloc[-1][service_columns]\n",
    "\n",
    "    # Calculate the difference between end and start values\n",
    "    energy_difference = end_values.values - start_values.values\n",
    "    # Sum up the differences\n",
    "    total_energy_sum = energy_difference.sum()\n",
    "    \n",
    "    # Convert to Joules\n",
    "    energy, unit = convert_energy(total_energy_sum)\n",
    "\n",
    "    return transform_joules(energy, unit, \"J\")\n",
    "\n",
    "def calculate_metrics(service_list, start_time, end_time):\n",
    "    metrics_results = {}\n",
    "    for service in service_list:\n",
    "        total_energy = calculate_service_energy_consumption(kepler_df, service, start_time, end_time)\n",
    "        \n",
    "        # Calculate successful request count from the service_work_metrics_df\n",
    "        service_work_metrics_filtered = service_work_metrics_df[(service_work_metrics_df.index >= start_time) & (service_work_metrics_df.index <= end_time)]\n",
    "        if service in set(map(str.lower,service_work_metrics_filtered.columns)):\n",
    "            total_requests = service_work_metrics_filtered[service.upper()].max() - service_work_metrics_filtered[service.upper()].min()\n",
    "        else:\n",
    "            total_requests = 0\n",
    "        \n",
    "        failed_requests = 0\n",
    "        \n",
    "        successful_requests = total_requests - failed_requests\n",
    "        efficiency = successful_requests / total_energy if total_energy != 0 else 0\n",
    "        \n",
    "        metrics_results[service] = {\n",
    "            'efficiency': efficiency,\n",
    "            'total_energy': total_energy,\n",
    "            'successful_requests': successful_requests\n",
    "        }\n",
    "    \n",
    "    return pd.DataFrame(metrics_results).transpose()\n",
    "\n",
    "service_metrics = calculate_metrics(service_names, start_time, end_time)\n",
    "service_metrics.to_latex(outputdir+'service_efficiency_energy_requests_table.tex', escape=True)\n",
    "service_metrics"
   ],
   "id": "14e923ac4ff179f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualisierung der Ergebnisse\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "filtered_service_metrics = service_metrics[service_metrics['efficiency'] > 0]\n",
    "ax.bar(filtered_service_metrics.index, filtered_service_metrics[\"efficiency\"])\n",
    "\n",
    "ax.set_xlabel('Services')\n",
    "ax.set_ylabel('Energy Efficiency (Success Requests / Joules)')\n",
    "#ax.set_title('Energy Efficiency per Service')\n",
    "ax.grid(True)\n",
    "\n",
    "plt.savefig(outputdir+'energy_efficiency_by_service.png', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "8c1437737673659e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### It makes sense to add the database energy consumption to each service as the generated load and useful done work is associated with this energy",
   "id": "5d22810eee8d89c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "combined_metrics = service_metrics[~service_metrics.index.str.endswith('-db')].copy()\n",
    "\n",
    "# Add DB energy consumption to the respective services\n",
    "combined_metrics['total_energy'] += combined_metrics.apply(\n",
    "    lambda row: service_metrics.loc[f\"{row.name}-db\", 'total_energy'] if f\"{row.name}-db\" in service_metrics.index else 0, axis=1\n",
    ")\n",
    "\n",
    "# Calculate efficiency\n",
    "combined_metrics['efficiency'] = combined_metrics['successful_requests'] / combined_metrics['total_energy']\n",
    "combined_metrics.to_latex(outputdir+'service_merged_with_database_efficiency_energy_requests_table.tex', escape=True)\n",
    "combined_metrics"
   ],
   "id": "69c692813757c62d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "filtered_combined_metrics = combined_metrics[combined_metrics['efficiency'] > 0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.bar(filtered_combined_metrics.index, filtered_combined_metrics[\"efficiency\"])\n",
    "ax.set_xlabel('Services')\n",
    "ax.set_ylabel('Energy Efficiency (Success Requests / Joules)')\n",
    "#ax.set_title('Energy Efficiency per Service (Including DB Consumption)')\n",
    "ax.grid(True)\n",
    "\n",
    "plt.savefig(outputdir+'energy_efficiency_by_service_merged_with_database.png', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "4c4ac8b262dd8efc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Calculate the energy efficiency of each service (including their DB) over each stage",
   "id": "3f64f0eef183a908"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to calculate energy consumption for each service during stages\n",
    "def calculate_combined_service_metrics_by_stage(service_list, stages, locust_df, kepler_df, service_work_metrics_df):\n",
    "    metrics_results_by_stage = {stage['users']: {} for stage in stages}\n",
    "\n",
    "    for stage in stages:\n",
    "        stage_users = stage['users']\n",
    "        stage_df = locust_df[locust_df['User Count'] == stage_users]\n",
    "\n",
    "        # Get the min and max index (time) for the current stage\n",
    "        if stage_df.empty:\n",
    "            logging.warning(\"No data available for stage \" + stage)\n",
    "            for service in service_list:\n",
    "                metrics_results_by_stage[stage_users][service] = {\n",
    "                    'efficiency': 0,\n",
    "                    'total_energy': 0,\n",
    "                    'successful_requests': 0,\n",
    "                }\n",
    "            continue\n",
    "\n",
    "        stage_start_time, stage_end_time = stage_df.index.min(), stage_df.index.max()\n",
    "        stage_start_time = stage_start_time.tz_convert(\"Europe/Berlin\").tz_localize(None)\n",
    "        stage_end_time = stage_end_time.tz_convert(\"Europe/Berlin\").tz_localize(None)\n",
    "\n",
    "        stage_work_metrics_df = service_work_metrics_df[\n",
    "            (service_work_metrics_df.index >= stage_start_time) & (service_work_metrics_df.index <= stage_end_time)]\n",
    "\n",
    "        for service in service_list:\n",
    "            if service.endswith('-db'):\n",
    "                continue  # Skip DB services\n",
    "\n",
    "            total_energy_service = calculate_service_energy_consumption(kepler_df, service, stage_start_time,\n",
    "                                                                        stage_end_time)\n",
    "            if f\"{service}-db\" in service_list:\n",
    "                total_energy_service += calculate_service_energy_consumption(kepler_df, f\"{service}-db\", stage_start_time, stage_end_time)\n",
    "\n",
    "            if service.upper() in set(map(str.upper, stage_work_metrics_df.columns)):\n",
    "                total_requests = stage_work_metrics_df[service.upper()].max() - stage_work_metrics_df[\n",
    "                    service.upper()].min()\n",
    "            else:\n",
    "                total_requests = 0\n",
    "\n",
    "            successful_requests = total_requests\n",
    "            efficiency = successful_requests / total_energy_service if total_energy_service != 0 else 0\n",
    "\n",
    "            metrics_results_by_stage[stage_users][service] = {\n",
    "                'efficiency': efficiency,\n",
    "                'total_energy': total_energy_service,\n",
    "                'successful_requests': successful_requests,\n",
    "            }\n",
    "\n",
    "    return metrics_results_by_stage\n",
    "\n",
    "\n",
    "service_metrics_by_stage = calculate_combined_service_metrics_by_stage(service_names, stages, locust_df, kepler_df,\n",
    "                                                                       service_work_metrics_df)\n",
    "service_metrics_by_stage\n",
    "\n",
    "# Convert dictionary to DataFrame for easier manipulation\n",
    "metrics_df_list = []\n",
    "\n",
    "for user_stage, service_metrics in service_metrics_by_stage.items():\n",
    "    df = pd.DataFrame(service_metrics).transpose()\n",
    "    df['user_stage'] = user_stage\n",
    "    metrics_df_list.append(df)\n",
    "\n",
    "staged_service_metrics_df = pd.concat(metrics_df_list)\n",
    "\n",
    "# Filter out services with zero efficiency\n",
    "staged_service_metrics_df = staged_service_metrics_df[staged_service_metrics_df['efficiency'] > 0]\n",
    "\n",
    "# Plot Efficiency of each combined service over stages\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "for service in staged_service_metrics_df.index.unique():\n",
    "    # Filter out rows for this service\n",
    "    filtered_df = staged_service_metrics_df.loc[staged_service_metrics_df.index == service]\n",
    "    ax.plot(filtered_df['user_stage'], filtered_df['efficiency'], label=service, marker='o', markersize=4)\n",
    "\n",
    "ax.set_xlabel('User Stage (User Count)')\n",
    "ax.set_ylabel('Energy Efficiency (Success Requests / Joules)')\n",
    "#ax.set_title('Energy Efficiency per Combined Service Over User Stages (Including DB Consumption)')\n",
    "ax.set_xticks([stage['users'] for stage in stages])\n",
    "ax.set_xticklabels([stage['users'] for stage in stages])\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(True)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig(outputdir+'energy_efficiency_by_service_by_stage.png', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "f3d9ace13938a4a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "logging.info(\"Finished\")",
   "id": "8f731eaa04a6176b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
